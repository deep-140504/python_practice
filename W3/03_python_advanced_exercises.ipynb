{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b8cfb3b-5256-488b-838e-75971c2a45ac",
   "metadata": {},
   "source": [
    "#### 1. Implement a python progtam to implement a multi threaded web scrapper the respects robot.txt rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb8fe23-07be-4628-9f02-01da39b4bf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "class RobotsParser:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "        self.allowed_paths = set()\n",
    "        self.disallowed_paths = set()\n",
    "        self.parse_robots_txt()\n",
    "\n",
    "    def parse_robots_txt(self):\n",
    "        robots_url = urljoin(self.base_url, \"robots.txt\")\n",
    "        try:\n",
    "            response = requests.get(robots_url, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                lines = response.text.splitlines()\n",
    "                for line in lines:\n",
    "                    if line.startswith(\"Disallow:\"):\n",
    "                        path = line[len(\"Disallow:\"):].strip()\n",
    "                        self.disallowed_paths.add(path)\n",
    "                    elif line.startswith(\"Allow:\"):\n",
    "                        path = line[len(\"Allow:\"):].strip()\n",
    "                        self.allowed_paths.add(path)\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"Failed to fetch robots.txt from {robots_url}. Assuming full access.\")\n",
    "\n",
    "    def is_allowed(self, url_path):\n",
    "        for disallowed_path in self.disallowed_paths:\n",
    "            if url_path.startswith(disallowed_path):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "class MultiThreadedScraper:\n",
    "    def __init__(self, base_url, max_threads=5):\n",
    "        self.base_url = base_url\n",
    "        self.visited_urls = set()  # URLs we've visited\n",
    "        self.queue = []  # URLs we still need to scrape\n",
    "        self.lock = threading.Lock()  # Lock for thread safety\n",
    "        self.robots_parser = RobotsParser(base_url)  # Respect robots.txt\n",
    "        self.max_threads = max_threads\n",
    "        self.found_urls = []  # List to store the scraped URLs\n",
    "\n",
    "    def fetch_url(self, url):\n",
    "        \"\"\" Fetch the content of the URL. \"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                return response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    def parse_links(self, html):\n",
    "        \"\"\" Parse the HTML to extract links. \"\"\"\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = set()\n",
    "        for a_tag in soup.find_all(\"a\", href=True):\n",
    "            full_url = urljoin(self.base_url, a_tag[\"href\"])\n",
    "            parsed_url = urlparse(full_url)\n",
    "            if parsed_url.netloc == urlparse(self.base_url).netloc:  # Same domain\n",
    "                links.add(full_url)\n",
    "        return links\n",
    "\n",
    "    def scrape_url(self, url):\n",
    "        \"\"\" Scrape the URL and extract links. \"\"\"\n",
    "        with self.lock:\n",
    "            if url in self.visited_urls:\n",
    "                return\n",
    "            self.visited_urls.add(url)\n",
    "\n",
    "        print(f\"Scraping: {url}\")\n",
    "        self.found_urls.append(url)  # Store the visited URL in found_urls\n",
    "        html = self.fetch_url(url)\n",
    "        if html:\n",
    "            links = self.parse_links(html)\n",
    "            with self.lock:\n",
    "                for link in links:\n",
    "                    if link not in self.visited_urls and self.robots_parser.is_allowed(urlparse(link).path):\n",
    "                        self.queue.append(link)\n",
    "\n",
    "    def worker(self):\n",
    "        \"\"\" Worker thread that processes the queue of URLs. \"\"\"\n",
    "        while True:\n",
    "            with self.lock:\n",
    "                if not self.queue:\n",
    "                    return\n",
    "                url = self.queue.pop(0)\n",
    "\n",
    "            self.scrape_url(url)\n",
    "\n",
    "    def save_to_csv(self, filename=\"scraped_urls.csv\"):\n",
    "        \"\"\" Save the found URLs to a CSV file. \"\"\"\n",
    "        with open(filename, \"w\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Scraped URLs\"])  # CSV Header\n",
    "            for url in self.found_urls:\n",
    "                writer.writerow([url])\n",
    "\n",
    "    def save_to_text_file(self, filename=\"scraped_urls.txt\"):\n",
    "        \"\"\" Save the found URLs to a text file. \"\"\"\n",
    "        with open(filename, \"w\") as file:\n",
    "            for url in self.found_urls:\n",
    "                file.write(url + \"\\n\")\n",
    "\n",
    "    def run(self, start_path=\"/\"):\n",
    "        \"\"\" Start scraping from the given start path. \"\"\"\n",
    "        self.queue.append(urljoin(self.base_url, start_path))\n",
    "        threads = []\n",
    "\n",
    "        for _ in range(self.max_threads):\n",
    "            t = threading.Thread(target=self.worker)\n",
    "            threads.append(t)\n",
    "            t.start()\n",
    "\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "\n",
    "        # After scraping, save the results to both CSV and text files\n",
    "        self.save_to_csv()  # Save URLs to CSV\n",
    "        self.save_to_text_file()  # Save URLs to text file\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = \"https://quotes.toscrape.com/\"  # Replace with the website URL you want to scrape\n",
    "    scraper = MultiThreadedScraper(base_url, max_threads=20)\n",
    "    scraper.run(start_path=\"/\")  # Start scraping from the homepage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b8cf45-85d9-4aa2-9c32-2d88693291fc",
   "metadata": {},
   "source": [
    "#### 5. Implement a thread safe priority queue"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e63f3f6-1b65-4744-a428-300981eba059",
   "metadata": {},
   "source": [
    "# Priority queue:\n",
    "- Priority queue are a data structure where operation is carried out on the basis of the priority\n",
    "- lower the value, higher the priority\n",
    "\n",
    "# Thread-Safe Data structure:\n",
    "- A thread safe data structures can safely handle access from multiple threads at the same time\n",
    "- without thread-safety simultaneous reads and writes can lead to race conditions and corrupted data\n",
    "- the queue.PriorityQueue class in python is inherently thread safe, meaning ou dont need to implement extra locking mechanism using it\n",
    "\n",
    "# Modules required:\n",
    "- threading: provides tools from creating and manging threads\n",
    "- PriorityQueue from queue: pre impleneted functionality relatid to threading and pririty queue\n",
    "- time: used to simulate delays\n",
    "- random: used to generate random priorities and delays for variety in the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81eea715-d58b-4e77-9497-b81568f6e9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producer-0 enqueueing: Item-0 from Producer-0 with priority 3\n",
      "Producer-1 enqueueing: Item-0 from Producer-1 with priority 5\n",
      "Consumer-0 dequeued: Item-0 from Producer-0 with priority 3\n",
      "Consumer-1 dequeued: Item-0 from Producer-1 with priority 5\n",
      "Consumer-1 waiting for items...\n",
      "Consumer-1 waiting for items...\n",
      "Producer-1 enqueueing: Item-1 from Producer-1 with priority 10\n",
      "Producer-0 enqueueing: Item-1 from Producer-0 with priority 9\n",
      "Consumer-0 dequeued: Item-1 from Producer-0 with priority 9\n",
      "Producer-1 enqueueing: Item-2 from Producer-1 with priority 8\n",
      "Consumer-1 dequeued: Item-2 from Producer-1 with priority 8\n",
      "Producer-1 enqueueing: Item-3 from Producer-1 with priority 8\n",
      "Producer-0 enqueueing: Item-2 from Producer-0 with priority 4\n",
      "Consumer-0 dequeued: Item-2 from Producer-0 with priority 4\n",
      "Producer-0 enqueueing: Item-3 from Producer-0 with priority 9\n",
      "Producer-1 enqueueing: Item-4 from Producer-1 with priority 8\n",
      "Consumer-1 dequeued: Item-3 from Producer-1 with priority 8\n",
      "Consumer-0 dequeued: Item-4 from Producer-1 with priority 8\n",
      "Producer-0 enqueueing: Item-4 from Producer-0 with priority 5\n",
      "Consumer-1 dequeued: Item-4 from Producer-0 with priority 5\n",
      "Consumer-0 dequeued: Item-3 from Producer-0 with priority 9\n",
      "Consumer-1 dequeued: Item-1 from Producer-1 with priority 10\n",
      "Consumer-0 waiting for items...\n",
      "Consumer-1 waiting for items...\n",
      "Consumer-0 waiting for items...\n",
      "Consumer-1 waiting for items...\n",
      "Consumer-1 waiting for items...\n",
      "Consumer-0 waiting for items...\n",
      "Consumer-1 waiting for items...\n",
      "Consumer-1 waiting for items...\n",
      "Consumer-0 waiting for items...\n",
      "Consumer-0 waiting for items...\n",
      "Consumer-1 waiting for items...\n",
      "Consumer-1 waiting for items...\n",
      "Consumer-0 waiting for items...\n",
      "Consumer-1 waiting for items...\n",
      "Consumer-0 waiting for items...\n",
      "Stopping consumers...\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "from queue import PriorityQueue\n",
    "import time\n",
    "import random\n",
    "\n",
    "class ThreadSafePriorityQueue:\n",
    "    def __init__(self):\n",
    "        # Initialize a PriorityQueue\n",
    "        self.pq = PriorityQueue()\n",
    "\n",
    "    def enqueue(self, priority, item):\n",
    "        \"\"\"Add an item to the priority queue with a given priority.\"\"\"\n",
    "        self.pq.put((priority, item))\n",
    "\n",
    "    def dequeue(self):\n",
    "        \"\"\"Remove and return the highest-priority item.\"\"\"\n",
    "        is_empty = self.is_empty()\n",
    "        if not is_empty:\n",
    "            return self.pq.get()\n",
    "        return None\n",
    "\n",
    "    def is_empty(self):\n",
    "        \"\"\"Check if the queue is empty.\"\"\"\n",
    "        return self.pq.empty()\n",
    "\n",
    "# Worker function for producer threads\n",
    "def producer(queue, producer_id):\n",
    "    for i in range(5):\n",
    "        priority = random.randint(1, 10)  # Generate a random priority\n",
    "        item = f\"Item-{i} from Producer-{producer_id}\"\n",
    "        print(f\"Producer-{producer_id} enqueueing: {item} with priority {priority}\")\n",
    "        queue.enqueue(priority, item)\n",
    "        time.sleep(random.random())  # Simulate variable processing time\n",
    "\n",
    "# Worker function for consumer threads\n",
    "def consumer(queue, consumer_id):\n",
    "    while True:\n",
    "        if not queue.is_empty():\n",
    "            priority, item = queue.dequeue()\n",
    "            print(f\"Consumer-{consumer_id} dequeued: {item} with priority {priority}\")\n",
    "        else:\n",
    "            print(f\"Consumer-{consumer_id} waiting for items...\")\n",
    "        time.sleep(random.random())  # Simulate variable processing time\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Create a thread-safe priority queue\n",
    "    priority_queue = ThreadSafePriorityQueue()\n",
    "\n",
    "    # Start producer threads\n",
    "    producers = [threading.Thread(target=producer, args=(priority_queue, i)) for i in range(2)]\n",
    "\n",
    "    # Start consumer threads\n",
    "    consumers = [threading.Thread(target=consumer, args=(priority_queue, i)) for i in range(2)]\n",
    "\n",
    "    # Start all threads\n",
    "    for p in producers:\n",
    "        p.start()\n",
    "    for c in consumers:\n",
    "        c.start()\n",
    "\n",
    "    # Wait for producers to finish\n",
    "    for p in producers:\n",
    "        p.join()\n",
    "\n",
    "    # Let consumers run for a while and then terminate\n",
    "    time.sleep(5)\n",
    "    print(\"Stopping consumers...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b155919-868d-440d-9a51-28abf5b005bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
